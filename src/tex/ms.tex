% Define document class
\documentclass[twocolumn]{aastex631}
\usepackage{showyourwork}
\usepackage{color}

\definecolor{rb4}{HTML}{27408B}
\newcommand{\kw}[1]{{\color{rb4}[KW: #1 ]}}

% Begin!
\begin{document}

% Title
\title{Turbo fast no compromise gravitational wave parameter estimation}

% Author list
\author{@kazewong}

% Abstract with filler text
\begin{abstract}
We do one thing and one thing only, gravitational wave parameters estimation, and we do it in turbo mode.
\end{abstract}

% Main body with filler text
\section{Introduction}
\label{sec:intro}

\section{Gravitational wave parameter estimation}

\subsection{MCMC}

\subsection{Waveform}

\subsection{Heterodyned likelihood}

The computational cost of evaluating a waveform model scales linearly with the number of sample points either in time or frequency domain.
This introduces greater computational burden for longer-duration signals.
To reduce the computational cost, there are a number of methods to reduce the number of basis points one would need to compute the likelihood faithfully \kw{cite}.
We use heterodyned likelihood \kw{cite} (also named relative binning in \kw{cite}) in this work.

\kw{Specify the idea of heterodyned likelihood here}.

\section{FlowMC}

Since our likelihood function is coded in 
\subsection{Gradient-based sampler}
\label{sec:gradient}

\texttt{Jax} allows us to compute the gradient of the likelihood function with respect to the parameters through automatic differentiation.
This enables us to use gradient-based samplers such as Metropolis-adjusted Langevin algorithm (MALA) \kw{cite} and Hamiltonian Monte Carlo (HMC)\cite{kw}.
The main advantage of gradient-based sampling methods over gradient-free sampling methods is the gradient information help decorrelates different parameters.
This means gradient-based sampling methods allow longer jump in a high dimensional space.
This is crucial for high dimensional problems \footnote{Gradient-free methods often have troubles dealing with problems with $>20$ dimensions.},
since gradient-free methods would need small step size to have reasonable acceptance rate in a high dimensional space,
which in turn leads to a more correlated samples hence slower convergence.

\subsection{Normalizing Flow}
\label{sec:flow}

While 

\subsection{Accelerators}
\label{sec:accelerators}


\section{Checks}

\begin{enumerate}
    \item ppplot
    \item real-event result
\end{enumerate}


\section{Discussion}

\kw{Talks about implication}
\begin{enumerate}
    \item Multi-modality in testing GR PE
    \item No pre-training, i.e. more adaptive to glitch and new noise model
    \item As gauruntee to converge as other MCMC method
    \item Access to higher dimensional problem
    \item Much lower computational cost, allow poor institute to just run PE on colab if they want.
\end{enumerate}

\kw{Talk about future development}


% Additional traits for having differentiable waveforms
There are a number of future developments we are working on.
While IMRPhenomD is a reasonable start, it lacks some qualitative features that other state-of-the-art models have,
such as precession and eccentricity \kw{cite}.
It also has higher mismatch with reference numerical relativity waveforms compared to other waveforms.
Currently, we are working on building differentiable IMRPhenomX and NRSurrogate waveforms.
Going forward, we encourage the waveform development community to leverage autodiff environment such as Jax when constructing new waveform models.
Having a differentiable waveform model is not only beneficial for parameter estimation, but also for other applications such as calibrating waveforms to numerical relativity result, as detailed in \kw{cite}.

% More features such as marginalization
The features we have implemented are the barebone version of parameter estimation.
We do not include marginalization schemes such as time, phase and calibration lines marginalization.
Because of the performance of the sampler on accelerators, time and phase marginalization is not necessary,
as the performance of our implementation is not significantly impacted by having two extra dimension.
Other marginalization mode should be considered in the future.

% JIT overhead
Jax's JIT compilation drastically reduce the computational time to evaluate the likelihood.
However, it comes with a compilation overhead when the likelihood is evaluated for the first time.
We observed the compilation time could depends on the device where the code will be run.
This is expected since Jax leverage Accelerated Linear Algebra (XLA) to take advantage of accelerators,
which means Jax needs to compile the code for the specific device according to its architecture.
On an Nvidia A100 GPU, the compilation overhead could go up to 5 minutes for the waveform we are using.
For the cases we have studied, the time needed to obtain converging results on a A100 is about 2-3 minutes.
This means the compilation overhead is dominating the wall-clock time of the specific PE run we considered.
To utilize our implementation to its full potential, we are looking into ways to reduce the compilation overhead,
or to cache the compilation results to avoid paying the compilation overhead for every events.

% Higher dimensional problems
While standard GW analysis goes up to 17 dimension, 
\begin{enumerate}
    \item Multi-device scaling?
    \item Other detectors configuration
    \item  Combining with noise inference
    \item Higher dimensional problems need HMC still
\end{enumerate}



\section{Acknowledgements}

\bibliography{bib}

\end{document}
