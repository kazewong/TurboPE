% Define document class
\documentclass[twocolumn]{aastex631}
\usepackage{showyourwork}
\usepackage{color}

\definecolor{rb4}{HTML}{27408B}
\newcommand{\kw}[1]{{\color{rb4}[KW: #1 ]}}

% Begin!
\begin{document}

% Title
\title{Turbo fast no compromise gravitational wave parameter estimation}

% Author list
\author{@kazewong}

% Abstract with filler text
\begin{abstract}
We present a light-weighted, flexible and high performance framework to estimate
gravitational wave event parameters. By combining heterodyned likelihood,
automatically differentiable and accelerators compatible waveforms, normali zing
flow enhanced gradient-based MCMC, we achieve parameter estimation for real
event like GW150914 and GW170817 within a minute of sampling time with little to
no compromise. Our framework does not require pre-training or explicit
reparameterization, and can be generalized to handle higher dimensional
problems. We also discuss as near-real time parameter estimation have been shown
to be possible by multiple groups, what are the trade-offs and future
developments the community should be aware of. Our code for generating the
manuscript and running the analysis is publically available on github.
\end{abstract}

% Main body with filler text
\section{Introduction}
\label{sec:intro}

\section{Gravitational wave parameter estimation}

In this section we give a self-contained summary the gravitational-wave parameter estimation process.

\subsection{MCMC}

\begin{align}
    p(\theta| d) = \frac{\mathcal{L}(d|\theta)p(\theta)}{p(d)}
\end{align}

\begin{align}
    \log{\mathcal{L}(d|\theta)} = -\left<d-h|d-h\right>/2
\end{align}

\begin{align}
    \left<a|b\right> = 4 Re\int \frac{a(f)^*b(f)}{\mathcal{S}_n(f)} df
\end{align}

\subsection{Waveform}

\subsection{Heterodyned likelihood}

The computational cost of evaluating a waveform model scales linearly with the
number of sample points either in time or frequency domain. This introduces
greater computational burden for longer-duration signals. To reduce the
computational cost, there are a number of methods to reduce the number of basis
points one would need to compute the likelihood faithfully \kw{cite}. We use
heterodyned likelihood \kw{cite} (also named relative binning in \kw{cite}) in
this work. We give a concise description of what we implement in our code here.
For more extensive discussion of the derivation of heterodyned likelihood, we
refer the reader the reference \kw{}.

\kw{Specify the idea of heterodyned likelihood here}.

\section{FlowMC}

In this section, we provide a high level summary of how each component in \texttt{flowMC} benefits the sampling process and their interplay.

Since our likelihood function is coded in 
\subsection{Gradient-based sampler}
\label{sec:gradient}

\texttt{Jax} allows us to compute the gradient of the likelihood function with
respect to the parameters through automatic differentiation. This enables us to
use gradient-based samplers such as Metropolis-adjusted Langevin algorithm
(MALA) \kw{cite} and Hamiltonian Monte Carlo (HMC)\cite{kw}. The main advantage
of gradient-based sampling methods over gradient-free sampling methods is the
gradient information help decorrelates different parameters. This means
gradient-based sampling methods allow longer jump in a high dimensional space.
This is crucial for high dimensional problems \footnote{Gradient-free methods
often have troubles dealing with problems with $>20$ dimensions.}, since
gradient-free methods would need small step size to have reasonable acceptance
rate in a high dimensional space, which in turn leads to a more correlated
samples hence slower convergence.

\kw{Show some examples of successful application of HMC}

\subsection{Normalizing Flow}
\label{sec:flow}

% Problem with just HMC or MALA
While gradient-based samplers have been shown to be superior in terms of
performance when compared to other gradient-free algorithms in many practical
examples\kw{cite}, there are still a number of classes of problems
gradient-based samplers do not solve well. \footnote{To be specific, we are
referring to sampling algorithms that use first order derivative here. Sampling
algorithms that use information of higher order derivative such as manifold-MALA
and Riemannian-HMC can in principle decorrelate local correlation in the target
distribution, however they often have instability issue when they are used on
real-life application, so they are not used often in practice.} For examples,
target distribution that exhibits local correlation are hard to dealt with,
since by construction first order gradient-based algorithms can only handle
global correlation structure. Another example is multi-modality. If there are
multiple modes in the target distribution, an independent chain will likely to
be trapped in one mode and take extremely long time to transverse between the
modes. This means the relative weights between modes will much longer to sample
compared to the shape of each mode.

% Long burn in too
Moreover, before we can use the sampling chain to estimate the quantity we care,
the sampler often needs to first find the most probable region in the target
space, which is a common process that is referred to "burn-in" in the
literature. This means one would discard a certain amount of data generated from
the beginning and only use the later part to estimate the quantities of
interest. The burn-in phase of a gradient-based sampler is often as long as the
sampling phase, which means a good portion of computation is not necessary
helpful in estimating the target quantities.

% Crux of normalizing flow
Normalizing flow is a neural-network based technique that aims at learning a
mapping between a simple distribution, such as a Gaussian distribution, to a
complex distribution, often given in the form of data samples from a target
distribution. Once the network is trained, one can evaluate the probability
density of the complex distribution and samples from it very efficiently.
The core of normalizing flow is given by
\begin{align}
    p_x(X) = p_z(Z) \prod_i \left| \frac{\partial f}{\partial x_i}\right|,
\end{align}
where $p_x(X)$ is the complex target distribution $p_z(Z)$ is the simple latent
distribution, and $f$ is a series of invertible parameterized transform that
connect the two distributions. For a detail discussion of the algorithm, we
refer the readers to \kw{cite}.

% How does it help our sampling
As mentioned in the previous subsection, one of the main issue gradient-based
sampler is it does not explore local correlation and multi-modality well. This
is exactly where the normalizing flow model can help. While the method would
still work when there is only one independent local sampler chain, this method
works much better when we employ a lot of independent chains. By looking across
multiple chains, the normalizing flow learns the global landscape of the target
distribution, hence we can use it as a \textit{proposal} distribution. This
means independent chains can now jump between modes, or within a single mode
that exhibits complex geometry that cannot be handled easily only through the
local sampler, which greatly reduce both the burn-in time and sampling time.

\kw{Mention how this helps gravitational wave, gives examples.}


\subsection{Accelerators}
\label{sec:accelerators}

Modern accelerators such as GPUs and TPUs are designed to execute large scale
dense computation. They are often much more cost-efficient than using many CPUs
when it comes to solving problems that can be benefited from parallelization.
The downside of these accelerators compared to CPUs is they can only perform a
more restricted set of operation, and are often less performant when they are
dealing with problem that are serial. Parameter estimation with MCMC is a serial
problem, since each new sample generated from a chain depends on the last sample
in the chain. This means naively putting the problem on an accelerator is more
likely to harm the performance instead of improving it.


% More sample helps training normalizing flow
In this work, the use of accelerators provides two independent perks that
tremendously benefits the parameter estimation process. First, using
accelerators allow us to run many independent chains at the same time, which
benefits the training of the normalizing flow. Since we generate the data we use
to train the normalizing flow on-the-fly, the more independent data we can feed
to the training process, the higher chance the normalizing flow can learn a
reasonable representation of the global landscape of the target distribution. If
we only use a small amount of chains, we are limited to the correlated samples
from each chain, so we have to run more sequential steps to get the same amount
of independent data as compared to running more chains, where former option does
not benefit from parallelization but the latter does. In another word, being
able to use many independent chains help the normalizing flow learn the global
landscape faster in WALL time.


% GPU helps pack a shit ton of waveform evaluation
Another benefit accelerator brings is parallel evaluation of waveforms. The most
expensive part of evaluating the likelihood is generating the waveform. This is
especially significant when analysis long signals such as BNS merger, since the
cost of evaluating the waveform scales linearly with the number of time samples.
While the most accurate waveform model, numerical relativity, is a sequential
process, most of the gravitational wave models used in PE are approximants,
meaning they are fits to numerical relativity waveforms. These fitted form can
be evaluated at any given time or frequency without the need to simulate the
signals from an initial condition. This means generating a waveform from an
approximant can in fact be benefited from parallelization. Together with
heterodyned likelihood, we can evaluate the likelihood at $\mathcal{O}(10^7)$
different locations on an Nvidia A100 GPU. The high throughput of likelihood
evaluations unlocks the potential of the \texttt{flowMC} sampling algorithms.

\section{Checks}

\begin{enumerate}
    \item ppplot
    \item real-event result
\end{enumerate}

\kw{GW159014}
\kw{GW170817}

\section{Discussion}

\kw{Talks about implication}
\begin{enumerate}
    \item Multi-modality in testing GR PE
    \item No pre-training, i.e. more adaptive to glitch and new noise model
    \item As gauruntee to converge as other MCMC method
    \item Access to higher dimensional problem
    \item Much lower computational cost, allow poor institute to just run PE on colab if they want.
\end{enumerate}

\kw{Talk about future development}


% Additional traits for having differentiable waveforms
There are a number of future developments we are working on. While IMRPhenomD is
a reasonable start, it lacks some qualitative features that other
state-of-the-art models have, such as precession and eccentricity \kw{cite}. It
also has higher mismatch with reference numerical relativity waveforms compared
to other waveforms. Currently, we are working on building differentiable
IMRPhenomX and NRSurrogate waveforms. Going forward, we encourage the waveform
development community to leverage autodiff environment such as Jax when
constructing new waveform models. Having a differentiable waveform model is not
only beneficial for parameter estimation, but also for other applications such
as calibrating waveforms to numerical relativity result, as detailed in
\kw{cite}.

% Precessing waveforms and higher mode.

% More features such as marginalization
The features we have implemented are the barebone version of parameter
estimation. We do not include marginalization schemes such as time, phase and
calibration lines marginalization. Because of the performance of the sampler on
accelerators, time and phase marginalization is not necessary, as the
performance of our implementation is not significantly impacted by having two
extra dimension. Other marginalization mode should be considered in the future.

% JIT overhead
Jax's JIT compilation drastically reduce the computational time to evaluate the
likelihood. However, it comes with a compilation overhead when the likelihood is
evaluated for the first time. We observed the compilation time could depends on
the device where the code will be run. This is expected since Jax leverage
Accelerated Linear Algebra (XLA) to take advantage of accelerators, which means
Jax needs to compile the code for the specific device according to its
architecture. On an Nvidia A100 GPU, the compilation overhead could go up to 5
minutes for the waveform we are using. For the cases we have studied, the time
needed to obtain converging results on a A100 is about 2-3 minutes. This means
the compilation overhead is dominating the wall-clock time of the specific PE
run we considered. To utilize our implementation to its full potential, we are
looking into ways to reduce the compilation overhead, or to cache the
compilation results to avoid paying the compilation overhead for every events.

% Higher dimensional problems
While standard GW analysis goes up to 17 dimensions, 

% Compared to other works

There are recent works from different groups on speeding up parameter estimation
of gravitational wave, including efficient reparameterization and deep learning
\kw{cite}. While all of these methods are pushing down minutes-scale parameter
estimation with high fidelity, we would like to highlight the unique strength of
this work. Compared to \kw{Stephan's paper}, we do not rely on pre-training the
neural network on a large collection of waveforms and noise realization. This
means as soon as new waveforms models and noise models are available, our
algorithm can already be deployed. Furthermore, our method is essentially an
MCMC algorithm, which inherits the merit of convergence measures in MCMC. As we
are only using the normalizing flow as a proposal distribution, and the
normalizing flow is trained jointly with a local sampler, we do not suffer from
overfitting since our data is always approaching the target distribution, in
this sense, we do not introduce potential extra systematic error to inference
result. Compared to \kw{Tousif's paper}, we do not rely on handcrafted
reparameterization of the coordinate systems. If the reparameterization scheme
is known ahead of time, a nice reparameterization is also encouraged. However,
handcrafted reparameterization depends on the assumptions used in deriving the
reparameterization scheme, which would inevitably run into limitations of use
cases. Our work can be viewed as an automatic reparameterization powered by
normalizing flow, while not being exact hence as efficient as an explicit
reparameterization, the method proposed in this work is applicable to a much
boarder class of problems, such as parameter estimation with precessing
waveforms, testing GR, and multi-event joint inference.



\begin{enumerate}
    \item Multi-device scaling?
    \item Other detectors configuration
    \item  Combining with noise inference
    \item Higher dimensional problems need HMC still
\end{enumerate}



\section{Acknowledgements}

\bibliography{bib}

\end{document}
